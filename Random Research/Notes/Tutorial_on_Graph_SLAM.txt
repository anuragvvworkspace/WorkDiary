Monocular Camera Localization in 3D LiDAR Maps

Abstract

Introduciton: multi-
path effects in urban canyons and does not provide suffi-
ciently accurate estimates indoors.

Many existing methods use the same sensor
type for mapping and localization. LiDARs provide accurate
range measurements, but are typically expensive and heavy	



ur method
exploits the advantages of both sensors by using a LiDAR for
mapping and a camera for localization.

Our method
exploits the advantages of both sensors by using a LiDAR for
mapping and a camera for localization. Today, map providers
already capture 3D LiDAR data to build large-scale maps.
Our method enables people to accurately localize themselves
in these maps without being equipped with a LiDAR.




The key idea of our work is to approach visual localization
by matching geometry




We argue that our approach is advantageous because the
geometry of the environment tends to be more stable than
its photometric appearance which can change tremendously
even over short periods


we present a method to track the 6-DoF
pose of a monocular camera in a 3D LiDAR map




local bundle
adjustment to reconstruct the camera poses relative to a
sparse set of 3D points. The main contribution of this
work is to continuously align this point set with the given
map by matching their geometry and applying an estimated
similarity transformation to indirectly localize the camera.


non-linear least
squares error minimization and solve it within an iterative
closest point procedure






Simultaneous
Localization and Mapping (SLAM) problem [15], [18]. How-
ever, the integration of both has mostly been done in the so-
called back-end of the SLAM process and not by matching
their data directly


### TO BE VERIFIED
Strasdat et al. [21] later showed that
keyframe-based optimization approaches outperform filtering
methods in terms of accuracy per computational cost.







